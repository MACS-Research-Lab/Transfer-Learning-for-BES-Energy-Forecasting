{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 11:51:29.040281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.constraints import MaxNorm\n",
    "\n",
    "import keras\n",
    "\n",
    "import os, sys\n",
    "rootpath = \"..\"\n",
    "sys.path.insert(0, f\"{os.getcwd()}/{rootpath}/base_models\")\n",
    "import model_prep\n",
    "\n",
    "step_back = 6  # window size = 6*5 = 30 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(building_name, tower_number, season, param_grid, use_delta=True, train_percentage=0.75, shuffle_seed=42):\n",
    "    features = ['FlowEvap', 'PerHumidity', 'TempAmbient', 'TempCondIn',\n",
    "       'TempCondOut', 'TempEvapIn', 'TempEvapOut', 'TempWetBulb',\n",
    "       'PerFreqConP', 'Tonnage','DayOfWeek', 'HourOfDay', 'PerFreqFan']\n",
    "    target = 'EnergyConsumption'\n",
    "\n",
    "    \"\"\"\n",
    "    1. Convert data into a model-compatible shape\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_df, _ = model_prep.create_preprocessed_lstm_df(\n",
    "        building_name=building_name,\n",
    "        tower_number=tower_number,\n",
    "        features=features,\n",
    "        target=target,\n",
    "        season=season,\n",
    "        use_delta=use_delta,\n",
    "    )\n",
    "    if not season:\n",
    "        season = \"allyear\"\n",
    "\n",
    "    \"\"\"\n",
    "    2. Split data into training and testing sets\n",
    "    \"\"\"\n",
    "\n",
    "    X = lstm_df.drop(f\"{target}(t)\", axis=1)  # drop target column\n",
    "    y = lstm_df[f\"{target}(t)\"]  # only have target column\n",
    "\n",
    "    # split into input and outputs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=(1 - train_percentage), shuffle=True, random_state=shuffle_seed\n",
    "    )\n",
    "\n",
    "    # scale feature data\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train[X_train.columns] = scaler.transform(X_train)\n",
    "    X_test[X_test.columns] = scaler.transform(X_test)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Get timestepped data as a 3D vector\n",
    "    \"\"\"\n",
    "    vec_X_train = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_train, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "    vec_X_test = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_test, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "\n",
    "    vec_y_train = y_train.values\n",
    "    vec_y_test = y_test.values\n",
    "\n",
    "    \"\"\"\n",
    "    4. Create and Train model\n",
    "    \"\"\"\n",
    "    # Create a function that builds the Keras model\n",
    "    def create_model(dropout_rate = 0.0, weight_constraint = 2.0, lstmcells = 32, activation = 'tanh', optimizer = \"Adamax\"):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(\n",
    "            keras.layers.LSTM(\n",
    "                lstmcells,\n",
    "                input_shape=(vec_X_train.shape[1], vec_X_train.shape[2]),\n",
    "                kernel_constraint=MaxNorm(weight_constraint),\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                activation=activation\n",
    "            )\n",
    "        )\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasRegressor(build_fn=create_model, loss=\"mse\", verbose=0)\n",
    "\n",
    "    # Create GridSearchCV and perform the grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid_result = grid_search.fit(vec_X_train, vec_y_train)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    print(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 11:51:36.698868: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.988032 using {'batch_size': 32, 'epochs': 200}\n",
      "0.929549 (0.041001) with: {'batch_size': 32, 'epochs': 50}\n",
      "0.983051 (0.002779) with: {'batch_size': 32, 'epochs': 100}\n",
      "0.988032 (0.001095) with: {'batch_size': 32, 'epochs': 200}\n",
      "0.861671 (0.001889) with: {'batch_size': 64, 'epochs': 50}\n",
      "0.967328 (0.015166) with: {'batch_size': 64, 'epochs': 100}\n",
      "0.986758 (0.001037) with: {'batch_size': 64, 'epochs': 200}\n",
      "0.829893 (0.003596) with: {'batch_size': 72, 'epochs': 50}\n",
      "0.972844 (0.003459) with: {'batch_size': 72, 'epochs': 100}\n",
      "0.986628 (0.000901) with: {'batch_size': 72, 'epochs': 200}\n",
      "0.458497 (0.002709) with: {'batch_size': 128, 'epochs': 50}\n",
      "0.889980 (0.028496) with: {'batch_size': 128, 'epochs': 100}\n",
      "0.983451 (0.000938) with: {'batch_size': 128, 'epochs': 200}\n",
      "GridSearchCV(cv=3,\n",
      "             estimator=KerasRegressor(build_fn=<function run_grid_search.<locals>.create_model at 0x7faf7086a670>, verbose=0),\n",
      "             param_grid={'batch_size': [32, 64, 72, 128],\n",
      "                         'epochs': [50, 100, 200]})\n"
     ]
    }
   ],
   "source": [
    "batch_size = [32, 64, 72, 128] # selecting 32\n",
    "epochs = [50, 100, 200]  # selecting 200\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "run_grid_search(building_name=\"ESB\", tower_number=1, season=\"summer\", param_grid=param_grid)\n",
    "\n",
    "# dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "# neurons = [6, 16, 32, 64, 128]\n",
    "# lstmcells = [1, 6, 16, 32, 64]\n",
    "# activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "# param_grid = dict(model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint)\n",
    "# param_grid = dict(model__activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(building_name, tower_number, season, param_grid, use_delta=True, train_percentage=0.75, shuffle_seed=42):\n",
    "    features = ['FlowEvap', 'PerHumidity', 'TempAmbient', 'TempCondIn',\n",
    "       'TempCondOut', 'TempEvapIn', 'TempEvapOut', 'TempWetBulb',\n",
    "       'PerFreqConP', 'Tonnage','DayOfWeek', 'HourOfDay', 'PerFreqFan']\n",
    "    target = 'EnergyConsumption'\n",
    "\n",
    "    \"\"\"\n",
    "    1. Convert data into a model-compatible shape\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_df, _ = model_prep.create_preprocessed_lstm_df(\n",
    "        building_name=building_name,\n",
    "        tower_number=tower_number,\n",
    "        features=features,\n",
    "        target=target,\n",
    "        season=season,\n",
    "        use_delta=use_delta,\n",
    "    )\n",
    "    if not season:\n",
    "        season = \"allyear\"\n",
    "\n",
    "    \"\"\"\n",
    "    2. Split data into training and testing sets\n",
    "    \"\"\"\n",
    "\n",
    "    X = lstm_df.drop(f\"{target}(t)\", axis=1)  # drop target column\n",
    "    y = lstm_df[f\"{target}(t)\"]  # only have target column\n",
    "\n",
    "    # split into input and outputs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=(1 - train_percentage), shuffle=True, random_state=shuffle_seed\n",
    "    )\n",
    "\n",
    "    # scale feature data\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train[X_train.columns] = scaler.transform(X_train)\n",
    "    X_test[X_test.columns] = scaler.transform(X_test)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Get timestepped data as a 3D vector\n",
    "    \"\"\"\n",
    "    vec_X_train = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_train, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "    vec_X_test = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_test, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "\n",
    "    vec_y_train = y_train.values\n",
    "    vec_y_test = y_test.values\n",
    "\n",
    "    \"\"\"\n",
    "    4. Create and Train model\n",
    "    \"\"\"\n",
    "    # Create a function that builds the Keras model\n",
    "    def create_model(dropout_rate = 0.0, weight_constraint = 2.0, lstmcells = 32, activation = 'tanh'):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(\n",
    "            keras.layers.LSTM(\n",
    "                lstmcells,\n",
    "                input_shape=(vec_X_train.shape[1], vec_X_train.shape[2]),\n",
    "                kernel_constraint=MaxNorm(weight_constraint),\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                activation=activation\n",
    "            )\n",
    "        )\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        return model\n",
    "\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasRegressor(build_fn=create_model, loss=\"mse\", batch_size=32, epochs=200, verbose=0)\n",
    "\n",
    "    # Create GridSearchCV and perform the grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid_result = grid_search.fit(vec_X_train, vec_y_train)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    print(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.989732 using {'optimizer': 'Adam'}\n",
      "-0.005965 (0.007950) with: {'optimizer': 'SGD'}\n",
      "0.988903 (0.001481) with: {'optimizer': 'RMSprop'}\n",
      "-0.758298 (0.003863) with: {'optimizer': 'Adagrad'}\n",
      "-0.766484 (0.012862) with: {'optimizer': 'Adadelta'}\n",
      "0.989732 (0.001529) with: {'optimizer': 'Adam'}\n",
      "0.988291 (0.000985) with: {'optimizer': 'Adamax'}\n",
      "GridSearchCV(cv=3,\n",
      "             estimator=KerasRegressor(batch_size=32, build_fn=<function run_grid_search.<locals>.create_model at 0x7faf744c1a60>, epochs=200, loss='mse', verbose=0),\n",
      "             param_grid={'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
      "                                       'Adam', 'Adamax']})\n"
     ]
    }
   ],
   "source": [
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax'] # selecting Adam\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "run_grid_search(building_name=\"ESB\", tower_number=1, season=\"summer\", param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(building_name, tower_number, season, param_grid, use_delta=True, train_percentage=0.75, shuffle_seed=42):\n",
    "    features = ['FlowEvap', 'PerHumidity', 'TempAmbient', 'TempCondIn',\n",
    "       'TempCondOut', 'TempEvapIn', 'TempEvapOut', 'TempWetBulb',\n",
    "       'PerFreqConP', 'Tonnage','DayOfWeek', 'HourOfDay', 'PerFreqFan']\n",
    "    target = 'EnergyConsumption'\n",
    "\n",
    "    \"\"\"\n",
    "    1. Convert data into a model-compatible shape\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_df, _ = model_prep.create_preprocessed_lstm_df(\n",
    "        building_name=building_name,\n",
    "        tower_number=tower_number,\n",
    "        features=features,\n",
    "        target=target,\n",
    "        season=season,\n",
    "        use_delta=use_delta,\n",
    "    )\n",
    "    if not season:\n",
    "        season = \"allyear\"\n",
    "\n",
    "    \"\"\"\n",
    "    2. Split data into training and testing sets\n",
    "    \"\"\"\n",
    "\n",
    "    X = lstm_df.drop(f\"{target}(t)\", axis=1)  # drop target column\n",
    "    y = lstm_df[f\"{target}(t)\"]  # only have target column\n",
    "\n",
    "    # split into input and outputs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=(1 - train_percentage), shuffle=True, random_state=shuffle_seed\n",
    "    )\n",
    "\n",
    "    # scale feature data\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train[X_train.columns] = scaler.transform(X_train)\n",
    "    X_test[X_test.columns] = scaler.transform(X_test)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Get timestepped data as a 3D vector\n",
    "    \"\"\"\n",
    "    vec_X_train = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_train, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "    vec_X_test = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_test, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "\n",
    "    vec_y_train = y_train.values\n",
    "    vec_y_test = y_test.values\n",
    "\n",
    "    \"\"\"\n",
    "    4. Create and Train model\n",
    "    \"\"\"\n",
    "    # Create a function that builds the Keras model\n",
    "    def create_model(activation, dropout_rate = 0.0, weight_constraint = 2.0, lstmcells = 32, optimizer = \"Adam\"):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(\n",
    "            keras.layers.LSTM(\n",
    "                lstmcells,\n",
    "                input_shape=(vec_X_train.shape[1], vec_X_train.shape[2]),\n",
    "                kernel_constraint=MaxNorm(weight_constraint),\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                activation=activation\n",
    "            )\n",
    "        )\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasRegressor(build_fn=create_model, loss=\"mse\", verbose=0)\n",
    "\n",
    "    # Create GridSearchCV and perform the grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid_result = grid_search.fit(vec_X_train, vec_y_train)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    print(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.978805 using {'model__activation': 'relu'}\n",
      "0.978805 (0.001613) with: {'model__activation': 'relu'}\n",
      "-0.823315 (0.017714) with: {'model__activation': 'tanh'}\n",
      "-0.921345 (0.028926) with: {'model__activation': 'sigmoid'}\n",
      "-0.915546 (0.024585) with: {'model__activation': 'hard_sigmoid'}\n",
      "0.977459 (0.001476) with: {'model__activation': 'linear'}\n",
      "GridSearchCV(cv=3,\n",
      "             estimator=KerasRegressor(build_fn=<function run_grid_search.<locals>.create_model at 0x7faf73a039d0>, loss='mse', verbose=0),\n",
      "             param_grid={'model__activation': ['relu', 'tanh', 'sigmoid',\n",
      "                                               'hard_sigmoid', 'linear']})\n"
     ]
    }
   ],
   "source": [
    "activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'] # selecting relu\n",
    "param_grid = dict(model__activation=activation)\n",
    "run_grid_search(building_name=\"ESB\", tower_number=1, season=\"summer\", param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(building_name, tower_number, season, param_grid, use_delta=True, train_percentage=0.75, shuffle_seed=42):\n",
    "    features = ['FlowEvap', 'PerHumidity', 'TempAmbient', 'TempCondIn',\n",
    "       'TempCondOut', 'TempEvapIn', 'TempEvapOut', 'TempWetBulb',\n",
    "       'PerFreqConP', 'Tonnage','DayOfWeek', 'HourOfDay', 'PerFreqFan']\n",
    "    target = 'EnergyConsumption'\n",
    "\n",
    "    \"\"\"\n",
    "    1. Convert data into a model-compatible shape\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_df, _ = model_prep.create_preprocessed_lstm_df(\n",
    "        building_name=building_name,\n",
    "        tower_number=tower_number,\n",
    "        features=features,\n",
    "        target=target,\n",
    "        season=season,\n",
    "        use_delta=use_delta,\n",
    "    )\n",
    "    if not season:\n",
    "        season = \"allyear\"\n",
    "\n",
    "    \"\"\"\n",
    "    2. Split data into training and testing sets\n",
    "    \"\"\"\n",
    "\n",
    "    X = lstm_df.drop(f\"{target}(t)\", axis=1)  # drop target column\n",
    "    y = lstm_df[f\"{target}(t)\"]  # only have target column\n",
    "\n",
    "    # split into input and outputs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=(1 - train_percentage), shuffle=True, random_state=shuffle_seed\n",
    "    )\n",
    "\n",
    "    # scale feature data\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train[X_train.columns] = scaler.transform(X_train)\n",
    "    X_test[X_test.columns] = scaler.transform(X_test)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Get timestepped data as a 3D vector\n",
    "    \"\"\"\n",
    "    vec_X_train = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_train, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "    vec_X_test = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_test, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "\n",
    "    vec_y_train = y_train.values\n",
    "    vec_y_test = y_test.values\n",
    "\n",
    "    \"\"\"\n",
    "    4. Create and Train model\n",
    "    \"\"\"\n",
    "    # Create a function that builds the Keras model\n",
    "    def create_model(lstmcells, activation=\"relu\", dropout_rate = 0.0, weight_constraint = 2.0, optimizer = \"Adam\"):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(\n",
    "            keras.layers.LSTM(\n",
    "                lstmcells,\n",
    "                input_shape=(vec_X_train.shape[1], vec_X_train.shape[2]),\n",
    "                kernel_constraint=MaxNorm(weight_constraint),\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                activation=activation\n",
    "            )\n",
    "        )\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasRegressor(build_fn=create_model, loss=\"mse\", verbose=0)\n",
    "\n",
    "    # Create GridSearchCV and perform the grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid_result = grid_search.fit(vec_X_train, vec_y_train)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    print(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.981176 using {'model__lstmcells': 64}\n",
      "0.976398 (0.002598) with: {'model__lstmcells': 16}\n",
      "0.977888 (0.002823) with: {'model__lstmcells': 32}\n",
      "0.981176 (0.000516) with: {'model__lstmcells': 64}\n",
      "0.980304 (0.002244) with: {'model__lstmcells': 128}\n",
      "GridSearchCV(cv=3,\n",
      "             estimator=KerasRegressor(build_fn=<function run_grid_search.<locals>.create_model at 0x7faf3187daf0>, loss='mse', verbose=0),\n",
      "             param_grid={'model__lstmcells': [16, 32, 64, 128]})\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(model__lstmcells=[16, 32, 64, 128]) # selecting 64\n",
    "run_grid_search(building_name=\"ESB\", tower_number=1, season=\"summer\", param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(building_name, tower_number, season, param_grid, use_delta=True, train_percentage=0.75, shuffle_seed=42):\n",
    "    features = ['FlowEvap', 'PerHumidity', 'TempAmbient', 'TempCondIn',\n",
    "       'TempCondOut', 'TempEvapIn', 'TempEvapOut', 'TempWetBulb',\n",
    "       'PerFreqConP', 'Tonnage','DayOfWeek', 'HourOfDay', 'PerFreqFan']\n",
    "    target = 'EnergyConsumption'\n",
    "\n",
    "    \"\"\"\n",
    "    1. Convert data into a model-compatible shape\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_df, _ = model_prep.create_preprocessed_lstm_df(\n",
    "        building_name=building_name,\n",
    "        tower_number=tower_number,\n",
    "        features=features,\n",
    "        target=target,\n",
    "        season=season,\n",
    "        use_delta=use_delta,\n",
    "    )\n",
    "    if not season:\n",
    "        season = \"allyear\"\n",
    "\n",
    "    \"\"\"\n",
    "    2. Split data into training and testing sets\n",
    "    \"\"\"\n",
    "\n",
    "    X = lstm_df.drop(f\"{target}(t)\", axis=1)  # drop target column\n",
    "    y = lstm_df[f\"{target}(t)\"]  # only have target column\n",
    "\n",
    "    # split into input and outputs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=(1 - train_percentage), shuffle=True, random_state=shuffle_seed\n",
    "    )\n",
    "\n",
    "    # scale feature data\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train[X_train.columns] = scaler.transform(X_train)\n",
    "    X_test[X_test.columns] = scaler.transform(X_test)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Get timestepped data as a 3D vector\n",
    "    \"\"\"\n",
    "    vec_X_train = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_train, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "    vec_X_test = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_test, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "\n",
    "    vec_y_train = y_train.values\n",
    "    vec_y_test = y_test.values\n",
    "\n",
    "    \"\"\"\n",
    "    4. Create and Train model\n",
    "    \"\"\"\n",
    "    # Create a function that builds the Keras model\n",
    "    def create_model(dropout_rate, weight_constraint, lstmcells=64, activation=\"relu\", optimizer = \"Adam\"):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(\n",
    "            keras.layers.LSTM(\n",
    "                lstmcells,\n",
    "                input_shape=(vec_X_train.shape[1], vec_X_train.shape[2]),\n",
    "                kernel_constraint=MaxNorm(weight_constraint),\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                activation=activation\n",
    "            )\n",
    "        )\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasRegressor(build_fn=create_model, loss=\"mse\", verbose=0)\n",
    "\n",
    "    # Create GridSearchCV and perform the grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid_result = grid_search.fit(vec_X_train, vec_y_train)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    print(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.980991 using {'model__dropout_rate': 0.0, 'model__weight_constraint': 4.0}\n",
      "0.980350 (0.001786) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 1.0}\n",
      "0.980659 (0.000954) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 2.0}\n",
      "0.977632 (0.003685) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 3.0}\n",
      "0.980991 (0.001114) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 4.0}\n",
      "0.979406 (0.003307) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 5.0}\n",
      "0.977514 (0.000792) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 1.0}\n",
      "0.978909 (0.000751) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 2.0}\n",
      "0.977338 (0.001237) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 3.0}\n",
      "0.980686 (0.000716) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 4.0}\n",
      "0.979798 (0.000870) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 5.0}\n",
      "0.979005 (0.001795) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 1.0}\n",
      "0.975689 (0.001128) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 2.0}\n",
      "0.978106 (0.002686) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 3.0}\n",
      "0.970191 (0.012112) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 4.0}\n",
      "0.976711 (0.002395) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 5.0}\n",
      "0.979614 (0.000622) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 1.0}\n",
      "0.976107 (0.004502) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 2.0}\n",
      "0.971977 (0.006636) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 3.0}\n",
      "0.979356 (0.000491) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 4.0}\n",
      "0.979675 (0.001159) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 5.0}\n",
      "0.965261 (0.008785) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 1.0}\n",
      "0.965165 (0.010641) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 2.0}\n",
      "0.973098 (0.002972) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 3.0}\n",
      "0.976912 (0.001518) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 4.0}\n",
      "0.976813 (0.002354) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 5.0}\n",
      "0.967111 (0.011359) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 1.0}\n",
      "0.976115 (0.004337) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 2.0}\n",
      "0.970596 (0.005060) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 3.0}\n",
      "0.966886 (0.014153) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 4.0}\n",
      "0.964847 (0.010787) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 5.0}\n",
      "0.973759 (0.005253) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 1.0}\n",
      "0.966715 (0.005802) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 2.0}\n",
      "0.964889 (0.007555) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 3.0}\n",
      "0.977101 (0.000387) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 4.0}\n",
      "0.949164 (0.003013) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 5.0}\n",
      "0.951697 (0.009360) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 1.0}\n",
      "0.964505 (0.008182) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 2.0}\n",
      "0.966247 (0.004491) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 3.0}\n",
      "0.957711 (0.005867) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 4.0}\n",
      "0.937309 (0.029333) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 5.0}\n",
      "0.937809 (0.027730) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 1.0}\n",
      "0.946416 (0.009707) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 2.0}\n",
      "0.885041 (0.006392) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 3.0}\n",
      "0.938167 (0.019266) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 4.0}\n",
      "0.913908 (0.009067) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 5.0}\n",
      "0.955524 (0.002291) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 1.0}\n",
      "0.931225 (0.015767) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 2.0}\n",
      "0.903216 (0.021236) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 3.0}\n",
      "0.946125 (0.015270) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 4.0}\n",
      "0.897853 (0.011207) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 5.0}\n",
      "GridSearchCV(cv=3,\n",
      "             estimator=KerasRegressor(build_fn=<function run_grid_search.<locals>.create_model at 0x7faf5530a430>, loss='mse', verbose=0),\n",
      "             param_grid={'model__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5,\n",
      "                                                 0.6, 0.7, 0.8, 0.9],\n",
      "                         'model__weight_constraint': [1.0, 2.0, 3.0, 4.0, 5.0]})\n"
     ]
    }
   ],
   "source": [
    "weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0] # select 0.0\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] # select 4.0\n",
    "param_grid = dict(model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint)\n",
    "run_grid_search(building_name=\"ESB\", tower_number=1, season=\"summer\", param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(building_name, tower_number, season, param_grid, use_delta=True, train_percentage=0.75, shuffle_seed=42):\n",
    "    features = ['FlowEvap', 'PerHumidity', 'TempAmbient', 'TempCondIn',\n",
    "       'TempCondOut', 'TempEvapIn', 'TempEvapOut', 'TempWetBulb',\n",
    "       'PerFreqConP', 'Tonnage','DayOfWeek', 'HourOfDay', 'PerFreqFan']\n",
    "    target = 'EnergyConsumption'\n",
    "\n",
    "    \"\"\"\n",
    "    1. Convert data into a model-compatible shape\n",
    "    \"\"\"\n",
    "\n",
    "    lstm_df, _ = model_prep.create_preprocessed_lstm_df(\n",
    "        building_name=building_name,\n",
    "        tower_number=tower_number,\n",
    "        features=features,\n",
    "        target=target,\n",
    "        season=season,\n",
    "        use_delta=use_delta,\n",
    "        step_back=6\n",
    "    )\n",
    "    if not season:\n",
    "        season = \"allyear\"\n",
    "\n",
    "    \"\"\"\n",
    "    2. Split data into training and testing sets\n",
    "    \"\"\"\n",
    "\n",
    "    X = lstm_df.drop(f\"{target}(t)\", axis=1)  # drop target column\n",
    "    y = lstm_df[f\"{target}(t)\"]  # only have target column\n",
    "\n",
    "    # split into input and outputs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=(1 - train_percentage), shuffle=True, random_state=shuffle_seed\n",
    "    )\n",
    "\n",
    "    # scale feature data\n",
    "    scaler = MinMaxScaler().fit(X_train)\n",
    "    X_train[X_train.columns] = scaler.transform(X_train)\n",
    "    X_test[X_test.columns] = scaler.transform(X_test)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Get timestepped data as a 3D vector\n",
    "    \"\"\"\n",
    "    vec_X_train = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_train, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "    vec_X_test = model_prep.df_to_3d(\n",
    "        lstm_dtframe=X_test, num_columns=len(features) + 1, step_back=step_back\n",
    "    )\n",
    "\n",
    "    vec_y_train = y_train.values\n",
    "    vec_y_test = y_test.values\n",
    "\n",
    "    \"\"\"\n",
    "    4. Create and Train model\n",
    "    \"\"\"\n",
    "    # Create a function that builds the Keras model\n",
    "    def create_model(step_back, dropout_rate=0.0, weight_constraint=4.0, lstmcells=64, activation=\"relu\", optimizer = \"Adam\"):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(\n",
    "            keras.layers.LSTM(\n",
    "                lstmcells,\n",
    "                input_shape=(vec_X_train.shape[1], vec_X_train.shape[2]),\n",
    "                kernel_constraint=MaxNorm(weight_constraint),\n",
    "                recurrent_dropout=dropout_rate,\n",
    "                activation=activation\n",
    "            )\n",
    "        )\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasRegressor(build_fn=create_model, loss=\"mse\", verbose=0)\n",
    "\n",
    "    # Create GridSearchCV and perform the grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3)\n",
    "    grid_result = grid_search.fit(vec_X_train, vec_y_train)\n",
    "\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    print(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "12\n",
      "12\n",
      "12\n",
      "24\n",
      "24\n",
      "24\n",
      "6\n",
      "Best: 0.980671 using {'model__step_back': 6}\n",
      "0.979950 (0.001641) with: {'model__step_back': 1}\n",
      "0.980481 (0.000253) with: {'model__step_back': 3}\n",
      "0.980671 (0.001003) with: {'model__step_back': 6}\n",
      "0.980418 (0.002116) with: {'model__step_back': 12}\n",
      "0.979959 (0.000832) with: {'model__step_back': 24}\n",
      "GridSearchCV(cv=3,\n",
      "             estimator=KerasRegressor(build_fn=<function run_grid_search.<locals>.create_model at 0x7faf56a7aee0>, loss='mse', verbose=0),\n",
      "             param_grid={'model__step_back': [1, 3, 6, 12, 24]})\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(model__step_back=[1, 3, 6, 12, 24]) # selecting step_back = 6\n",
    "\n",
    "# Call the run_grid_search function with the modified param_grid\n",
    "run_grid_search(building_name=\"ESB\", tower_number=1, season=\"summer\", param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "batch size: 32\n",
    "epochs: 200\n",
    "optimizer: Adam\n",
    "dropout rate: 0.0\n",
    "weight constraints: 4.0\n",
    "step_back: 6\n",
    "lstm cells: 64\n",
    "activation: relu\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
