{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "from typing import List\n",
    "import os, sys, time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "\n",
    "rootpath = \"..\"\n",
    "sys.path.insert(0, f\"{os.getcwd()}/{rootpath}/base_models\")\n",
    "sys.path.insert(0, f\"{os.getcwd()}/{rootpath}/source_models\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import model_prep\n",
    "\n",
    "\n",
    "step_back = 6  # window size = 6*5 = 30 mins\n",
    "\n",
    "step_back = 6  # window size = 6*5 = 30 mins\n",
    "season_map = {\n",
    "    \"spring\": [3, 4, 5],\n",
    "    \"summer\": [6, 7, 8],\n",
    "    \"fall\": [9, 10, 11],\n",
    "    \"winter\": [12, 1, 2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_building_name = \"ESB\"\n",
    "from_tower_number = 1\n",
    "to_building_name = \"ESB\"\n",
    "to_tower_number = 2\n",
    "features = ['FlowEvap', 'PerHumidity', 'TempAmbient', 'TempCondIn',\n",
    "    'TempCondOut', 'TempEvapIn', 'TempEvapOut', 'TempWetBulb',\n",
    "    'PerFreqConP', 'Tonnage', 'PerFreqFan']\n",
    "target = 'EnergyConsumption'\n",
    "to_season = \"summer\"\n",
    "from_season = \"summer\"\n",
    "finetuning_percentage = 0.8\n",
    "source_epochs=100\n",
    "finetune_epochs = 100\n",
    "display_results = True\n",
    "use_delta = True\n",
    "shuffle_seed = 42\n",
    "train_percentage = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Load data and do basic preprocessing\n",
    "\"\"\"\n",
    "# load data\n",
    "df = pd.read_csv(\n",
    "    f\"{rootpath}/data/{from_building_name.lower()}/{from_building_name.lower()}{from_tower_number}_preprocessed.csv\",\n",
    "    index_col=\"time\",\n",
    ")\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# only take data for one season\n",
    "df = model_prep.choose_season(df, season=from_season)\n",
    "\n",
    "# remove cases in which tower was OFF, and cases where OFF data would be included in past timesteps of ON data\n",
    "on_condition = df[target] > 0\n",
    "df = df.drop(df[~on_condition].index, axis=0)\n",
    "\n",
    "# select features and targets and create final dataframe that includes only relevant features and targets\n",
    "df = df[features+[\"DayOfWeek\"]].join(df[target], on=df.index)\n",
    "\n",
    "# if difference from first value should be used as for predictions then return the first value\n",
    "first_val = df.iloc[0, df.columns.get_loc(target)]\n",
    "if use_delta:\n",
    "    df[target] = (\n",
    "        df[target] - first_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2a. Trend removal on target\n",
    "\"\"\"\n",
    "def calculate_tsi(time_series, m):\n",
    "    \"\"\"\n",
    "    Calculate ts_i for each data point i in the consumption time series.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: Pandas Series representing the consumption time series.\n",
    "    - m: Integer representing the number of data points to consider before i.\n",
    "\n",
    "    Returns:\n",
    "    - Pandas Series containing ts_i values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the calculated ts_i values\n",
    "    ts_i_values = []\n",
    "\n",
    "    # Iterate through the time series\n",
    "    for i in range(len(time_series)):\n",
    "        if i < m:\n",
    "            # For the first m data points, use the mean of available data\n",
    "            ts_i = time_series.iloc[:i + 1].mean()\n",
    "        else:\n",
    "            # For subsequent data points, calculate ts_i using the formula\n",
    "            ts_i = 1 / m * time_series.iloc[i - m + 1:i + 1].sum()\n",
    "\n",
    "        # Append the calculated ts_i value to the list\n",
    "        ts_i_values.append(ts_i)\n",
    "\n",
    "    # Create a Pandas Series from the list of ts_i values\n",
    "    ts_i_series = pd.Series(ts_i_values, index=time_series.index, name='ts_i')\n",
    "\n",
    "    return ts_i_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def invert_tsi(results, m, old_df_len):\n",
    "#     \"\"\"\n",
    "#     Calculate ts_i for each data point i in the consumption time series.\n",
    "\n",
    "#     Parameters:\n",
    "#     - time_series: Pandas Series representing the consumption time series.\n",
    "#     - m: Integer representing the number of data points to consider before i.\n",
    "\n",
    "#     Returns:\n",
    "#     - Pandas Series containing ts_i values.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Initialize an empty list to store the calculated ts_i values\n",
    "#     ts_i_values = []\n",
    "\n",
    "#     # Iterate through the time series\n",
    "#     for i in range(len(results)):\n",
    "#         if i < m:\n",
    "#             # For the first m data points, use the mean of available data\n",
    "#             ts_i = results.iloc[:i + 1].mean()\n",
    "#         else:\n",
    "#             # For subsequent data points, calculate ts_i using the formula\n",
    "#             ts_i = 1 / m * time_series.iloc[i - m + 1:i + 1].sum()\n",
    "\n",
    "#         # Append the calculated ts_i value to the list\n",
    "#         ts_i_values.append(ts_i)\n",
    "\n",
    "#     # Create a Pandas Series from the list of ts_i values\n",
    "#     ts_i_series = pd.Series(ts_i_values, index=time_series.index, name='ts_i')\n",
    "\n",
    "#     return ts_i_series\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2b. Seasonality removal\n",
    "\"\"\"\n",
    "\n",
    "def calculate_seasonal_index(time_series, seasonality_column, m):\n",
    "    \"\"\"\n",
    "    Calculate the seasonal index for each seasonality value in the time series.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: Pandas DataFrame containing the time series data with a column for the seasonality values.\n",
    "    - seasonality_column: String representing the column name containing the seasonality values (e.g., days of the week).\n",
    "    - m: Integer representing the number of data points for each seasonality value.\n",
    "\n",
    "    Returns:\n",
    "    - Pandas DataFrame containing the seasonal index for each seasonality value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Group the data by the seasonality column\n",
    "    grouped_data = time_series.groupby(seasonality_column)\n",
    "\n",
    "    # Calculate the average of all target variable data points\n",
    "    y_bar = time_series.mean()[target]\n",
    "\n",
    "    # Initialize an empty dictionary to store the seasonal index values\n",
    "    seasonal_index_dict = {}\n",
    "\n",
    "    # Iterate through each group (seasonality value)\n",
    "    for group, group_data in grouped_data:\n",
    "        # Calculate the sum of the first m data points\n",
    "        sum_y_p_j = group_data.iloc[:m][target].sum()\n",
    "\n",
    "        # Calculate the seasonal index using the provided formula\n",
    "        seasonal_index = 1 / y_bar * (1 / m) * sum_y_p_j\n",
    "\n",
    "        # Store the seasonal index value in the dictionary\n",
    "        seasonal_index_dict[group] = seasonal_index\n",
    "\n",
    "    # Convert the dictionary to a Pandas DataFrame\n",
    "    seasonal_index_df = pd.DataFrame(list(seasonal_index_dict.items()), columns=[seasonality_column, 'sp'])\n",
    "\n",
    "    return seasonal_index_df\n",
    "\n",
    "def operate_with_sp(col, sp_df, operation):\n",
    "    index_col = col.index\n",
    "    combined_df = pd.merge(col, sp_df, left_on=col.index.dayofweek, right_on='DayOfWeek', how='left').set_index(index_col)\n",
    "    if operation == 'multiply':\n",
    "        combined_df[col.name] = combined_df[col.name] * combined_df['sp']\n",
    "    elif operation == 'divide':\n",
    "        combined_df[col.name] = combined_df[col.name] / combined_df['sp']\n",
    "    else:\n",
    "        raise ValueError('Invalid operation')\n",
    "    return combined_df[col.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply trend removal: UNDONE\n",
    "# m = 7*int(24*60/5) # FIXME: choose this carefully\n",
    "# df['ts_i'] = calculate_tsi(df[target], m)\n",
    "# df['tf_i'] = df['ts_i'] # chose len(df), bcs paper says to pick last index but my indices are time objects\n",
    "\n",
    "# df[target] = df[target] / df['tf_i']\n",
    "\n",
    "# apply seasonality removal\n",
    "sdf = calculate_seasonal_index(df, 'DayOfWeek', 7)\n",
    "df[target] = operate_with_sp(df[target], sdf, 'divide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Split data into training and testing sets\n",
    "\"\"\"\n",
    "\n",
    "df = df.dropna() # drop first NaN value due to zero division\n",
    "X = df[features]  # only have features\n",
    "y = df[target]  # only have target column\n",
    "\n",
    "# split into input and outputs\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=(1 - train_percentage), shuffle=False, random_state=shuffle_seed\n",
    ")\n",
    "\n",
    "# scale feature data\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train[X_train.columns] = scaler.transform(X_train)\n",
    "X_test[X_test.columns] = scaler.transform(X_test)\n",
    "vec_X_train = X_train.values\n",
    "vec_X_test = X_test.values\n",
    "\n",
    "\n",
    "vec_y_train = y_train.values\n",
    "vec_y_test = y_test.values\n",
    "# scaler_y = MinMaxScaler().fit(y_train.values.reshape(-1, 1))\n",
    "# vec_y_train = scaler_y.transform(y_train.values.reshape(-1, 1))\n",
    "# vec_y_test = scaler_y.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Build the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(len(features),), kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0) # FIXME: optimize hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(vec_X_test)\n",
    "\n",
    "# Inverse transform to get predictions in the original scale\n",
    "# y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "# vec_y_test = scaler_y.inverse_transform(vec_y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"actual\": vec_y_test.reshape((vec_y_test.shape[0])),\n",
    "        \"predicted\": y_pred.reshape((y_pred.shape[0])),\n",
    "    },\n",
    "    index=y_test.index,\n",
    ")\n",
    "\n",
    "# invert seasonality removal\n",
    "results_df['actual'] = operate_with_sp(results_df['actual'], sdf, 'multiply')\n",
    "results_df['predicted'] = operate_with_sp(results_df['predicted'], sdf, 'multiply')\n",
    "\n",
    "# invert trend removal UNDONE\n",
    "# merged_df = pd.merge(results_df, df['tf_i'], left_on=results_df.index, right_on=df['tf_i'].index, how='left').set_index(results_df.index)\n",
    "# merged_df['actual'] = merged_df['actual'] * merged_df['tf_i']\n",
    "# merged_df['predicted'] = merged_df['predicted'] * merged_df['tf_i']\n",
    "# merged_df.drop(columns=[col for col in merged_df.columns if col not in [\"actual\", \"predicted\"]], inplace=True)\n",
    "# results_df = merged_df\n",
    "\n",
    "mae = mean_absolute_error(results_df['actual'], results_df['predicted'])\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "display_df = pd.DataFrame(\n",
    "    index=pd.date_range(\n",
    "        start=results_df.index.min(), end=results_df.index.max(), freq=\"5min\"\n",
    "    )\n",
    ").merge(results_df, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "fig = px.line(display_df, x=display_df.index, y=[\"actual\", \"predicted\"])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot with reduced opacity\n",
    "ax = results_df.plot(alpha=0.5)\n",
    "\n",
    "# Customize the plot if needed (e.g., add labels, title, etc.)\n",
    "ax.set_xlabel('X-axis Label')\n",
    "ax.set_ylabel('Y-axis Label')\n",
    "ax.set_title('Title')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Load data and do basic preprocessing\n",
    "\"\"\"\n",
    "# load data\n",
    "df = pd.read_csv(\n",
    "    f\"{rootpath}/data/{to_building_name.lower()}/{to_building_name.lower()}{to_tower_number}_preprocessed.csv\",\n",
    "    index_col=\"time\",\n",
    ")\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# only take data for one season\n",
    "df = model_prep.choose_season(df, season=from_season)\n",
    "\n",
    "# remove cases in which tower was OFF, and cases where OFF data would be included in past timesteps of ON data\n",
    "on_condition = df[target] > 0\n",
    "df = df.drop(df[~on_condition].index, axis=0)\n",
    "\n",
    "# select features and targets and create final dataframe that includes only relevant features and targets\n",
    "df = df[features + [\"DayOfWeek\"]].join(df[target], on=df.index)\n",
    "\n",
    "# if difference from first value should be used as for predictions then return the first value\n",
    "first_val = df.iloc[0, df.columns.get_loc(target)]\n",
    "if use_delta:\n",
    "    df[target] = (\n",
    "        df[target] - first_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Split data into training and testing sets\n",
    "\"\"\"\n",
    "\n",
    "# Calculate the index to split the data (% training, % testing)\n",
    "num_rows = len(df)\n",
    "split_index = int(finetuning_percentage * num_rows)\n",
    "\n",
    "# Split the data\n",
    "train_set = df.iloc[:split_index]\n",
    "test_set = df.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply trend removal UNDONE\n",
    "# m = 7*int(24*60/5) # FIXME: choose this carefully\n",
    "# train_set['ts_i'] = calculate_tsi(train_set[target], m)\n",
    "# tf_len_multiplier = 1 # len(train_set)\n",
    "# train_set['tf_i'] = train_set['ts_i'] # EDITT len(train_set) # chose len(df), bcs paper says to pick last index but my indices are time objects\n",
    "# train_set[target] = train_set[target] / train_set['tf_i']\n",
    "\n",
    "# apply seasonality removal\n",
    "sdf = calculate_seasonal_index(train_set, 'DayOfWeek', 7)\n",
    "train_set[target] = operate_with_sp(col=train_set[target], sp_df=sdf, operation=\"divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values due to trend/seasonality removal\n",
    "train_set = train_set.dropna()\n",
    "test_set = test_set.dropna()\n",
    "\n",
    "# Split further\n",
    "X_train, X_test = train_set[features], test_set[features]\n",
    "y_train, y_test = train_set[target], test_set[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale feature data - use source domain scaler\n",
    "X_train[X_train.columns] = scaler.transform(X_train)\n",
    "X_test[X_test.columns] = scaler.transform(X_test)\n",
    "vec_X_train = X_train.values\n",
    "vec_X_test = X_test.values\n",
    "\n",
    "\n",
    "vec_y_train = y_train.values\n",
    "vec_y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(vec_X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"actual\": vec_y_test.reshape((vec_y_test.shape[0])),\n",
    "        \"predicted\": y_pred.reshape((y_pred.shape[0])),\n",
    "    },\n",
    "    index=y_test.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert seasonality removal\n",
    "results_df['actual'] = operate_with_sp(results_df['actual'], sdf, 'multiply')\n",
    "results_df['predicted'] = operate_with_sp(results_df['predicted'], sdf, 'multiply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # invert trend removal UNDONE\n",
    "# def recover_original_values(dtframe, target_column, m, start_index, tf_multiplier):\n",
    "#     # Create a new column to store the recovered original values\n",
    "#     recovered_column = f\"recovered_{target_column}\"\n",
    "#     dtframe[recovered_column] = dtframe[target_column]\n",
    "\n",
    "#     for i, (index, _) in enumerate(dtframe.loc[start_index:].iterrows()):\n",
    "        \n",
    "#         # Calculate the sum of the past m-1 values\n",
    "#         past_values_sum = dtframe[target_column].iloc[i - m + 1:i].sum()\n",
    "#         # Calculate the recovered original value by subtracting the sum from the average\n",
    "#         recovered_value = dtframe[target_column].iloc[i] * m - past_values_sum\n",
    "#         # Update the recovered column with the original value\n",
    "#         dtframe.at[index, recovered_column] = recovered_value\n",
    "    \n",
    "#     dtframe[f\"recovered_{target_column}\"] = dtframe[f\"recovered_{target_column}\"] / tf_multiplier\n",
    "\n",
    "#     return dtframe\n",
    "\n",
    "\n",
    "# pred_set = test_set.copy()\n",
    "# pred_set[target] = results_df[\"predicted\"]\n",
    "# start_of_predictions = pred_set.index[0]\n",
    "# combined_train_pred = train_set.append(pred_set, ignore_index=False)\n",
    "# rec_train_pred = recover_original_values(combined_train_pred, target, 7, start_of_predictions, tf_len_multiplier)\n",
    "# results_df['predicted'] = rec_train_pred[start_of_predictions:][f\"recovered_{target}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(results_df['actual'], results_df['predicted'])\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# Plot the results\n",
    "display_df = pd.DataFrame(\n",
    "    index=pd.date_range(\n",
    "        start=results_df.index.min(), end=results_df.index.max(), freq=\"5min\"\n",
    "    )\n",
    ").merge(results_df, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "fig = px.line(display_df, x=display_df.index, y=[\"actual\", \"predicted\"])\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
